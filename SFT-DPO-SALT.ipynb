{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cb4af4-9aef-4783-8765-f47d731cfe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5c0711-89fd-4727-882e-054080cc042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global debug\n",
    "debug = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39055ed0-f1bf-4f59-8e25-77265c96c6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 26 13:56:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 40%   45C    P8    20W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a80afd-66fc-47f0-ad8a-2d290f3c26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 0. imports\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, load_from_disk, load_metric\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "from transformers.trainer_utils import EvalPrediction# , EvalLoopOutput\n",
    "# from transformers.trainer_pt_utils import find_batch_size, nested_concat\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sequence_alignment import needle, smith, core\n",
    "from sequence_alignment.needle import get_position_status\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ecfe1b-d91e-4d6c-a243-7326a5b6cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Authors: Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn 2023\n",
    "# Copyright 2023 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollator, PreTrainedModel, PreTrainedTokenizerBase, Trainer, TrainingArguments\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "from trl.import_utils import is_peft_available\n",
    "# from trl.trainer.utils import DPODataCollatorWithPadding, pad_to_length\n",
    "from trl.trainer.utils import pad_to_length\n",
    "\n",
    "if is_peft_available():\n",
    "    from peft import get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca458788-cebc-44e4-aeb8-6adf4007b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import Rouge, AutomaticNgramEval, AutomaticFactEval\n",
    "\n",
    "ngram_eval = AutomaticNgramEval()\n",
    "# factev = AutomaticFactEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8122b98-0843-4325-b6a0-b0376e75ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DPODataCollatorWithPadding:\n",
    "    r\"\"\"\n",
    "    DPO DataCollator class that pads the inputs to the maximum length of the batch.\n",
    "    Args:\n",
    "        tokenizer (`PreTrainedTokenizerBase`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`Union[bool, str, `PaddingStrategy`]`, `optional`, defaults to `True`):\n",
    "            padding_strategy to pass to the tokenizer.\n",
    "        max_length (`Optional[int]`, `optional`, defaults to `None`):\n",
    "            The maximum length of the sequence to be processed.\n",
    "        max_prompt_length (`Optional[int]`, `optional`, defaults to `None`):\n",
    "            The maximum length of the prompt to be processed.\n",
    "        label_pad_token_id (`int`, defaults to -100):\n",
    "            The label used for masking.\n",
    "        padding_value (`int`, defaults to 0):\n",
    "            The value used for padding.\n",
    "        truncation_mode: (`str`, defaults to \"keep_end\"):\n",
    "            The truncation mode to use when truncating the prompt + chosen/rejected responses.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_prompt_length: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    padding_value: int = 0\n",
    "    truncation_mode: str = \"keep_end\"\n",
    "\n",
    "    def tokenize_batch_element(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        chosen: str,\n",
    "        rejected: str,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Tokenize a single batch element.\n",
    "\n",
    "        At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n",
    "            in case the prompt + chosen or prompt + rejected responses is/are too long. First\n",
    "            we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n",
    "\n",
    "        We also create the labels for the chosen/rejected responses, which are of length equal to\n",
    "            the sum of the length of the prompt and the chosen/rejected response, with\n",
    "            label_pad_token_id  for the prompt tokens.\n",
    "        \"\"\"\n",
    "        chosen_tokens = self.tokenizer(chosen, add_special_tokens=False)\n",
    "        rejected_tokens = self.tokenizer(rejected, add_special_tokens=False)\n",
    "        prompt_tokens = self.tokenizer(prompt, add_special_tokens=False)\n",
    "\n",
    "        assert self.tokenizer.eos_token_id not in prompt_tokens[\"input_ids\"], f\"Prompt contains EOS token: {prompt}\"\n",
    "        assert (\n",
    "            self.tokenizer.eos_token_id not in chosen_tokens[\"input_ids\"]\n",
    "        ), f\"Chosen response contains EOS token: {chosen}\"\n",
    "        assert (\n",
    "            self.tokenizer.eos_token_id not in rejected_tokens[\"input_ids\"]\n",
    "        ), f\"Rejected response contains EOS token: {rejected}\"\n",
    "\n",
    "        chosen_tokens[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "        chosen_tokens[\"attention_mask\"].append(1)\n",
    "\n",
    "        rejected_tokens[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "        rejected_tokens[\"attention_mask\"].append(1)\n",
    "\n",
    "        longer_response_length = max(len(chosen_tokens[\"input_ids\"]), len(rejected_tokens[\"input_ids\"]))\n",
    "\n",
    "        # if combined sequence is too long, truncate the prompt\n",
    "        if len(prompt_tokens[\"input_ids\"]) + longer_response_length > self.max_length:\n",
    "            if self.truncation_mode == \"keep_start\":\n",
    "                prompt_tokens = {k: v[: self.max_prompt_length] for k, v in prompt_tokens.items()}\n",
    "            elif self.truncation_mode == \"keep_end\":\n",
    "                prompt_tokens = {k: v[-self.max_prompt_length :] for k, v in prompt_tokens.items()}\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown truncation mode: {self.truncation_mode}\")\n",
    "\n",
    "        # if that's still too long, truncate the response\n",
    "        if len(prompt_tokens[\"input_ids\"]) + longer_response_length > self.max_length:\n",
    "            chosen_tokens = {k: v[: self.max_length - self.max_prompt_length] for k, v in chosen_tokens.items()}\n",
    "            rejected_tokens = {k: v[: self.max_length - self.max_prompt_length] for k, v in rejected_tokens.items()}\n",
    "            \n",
    "        # Create labels\n",
    "        chosen_sequence_tokens = {k: prompt_tokens[k] + chosen_tokens[k] for k in chosen_tokens}\n",
    "        rejected_sequence_tokens = {k: prompt_tokens[k] + rejected_tokens[k] for k in rejected_tokens}\n",
    "        chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n",
    "        chosen_sequence_tokens[\"labels\"][: len(prompt_tokens[\"input_ids\"])] = [self.label_pad_token_id] * len(\n",
    "            prompt_tokens[\"input_ids\"]\n",
    "        )\n",
    "        rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n",
    "        rejected_sequence_tokens[\"labels\"][: len(prompt_tokens[\"input_ids\"])] = [self.label_pad_token_id] * len(\n",
    "            prompt_tokens[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "        batch = {}\n",
    "\n",
    "        batch[\"prompt\"] = prompt\n",
    "        batch[\"chosen\"] = prompt + chosen\n",
    "        batch[\"rejected\"] = prompt + rejected\n",
    "        batch[\"chosen_response_only\"] = chosen\n",
    "        batch[\"rejected_response_only\"] = rejected\n",
    "\n",
    "        for k, toks in {\n",
    "            \"chosen\": chosen_sequence_tokens,\n",
    "            \"rejected\": rejected_sequence_tokens,\n",
    "            \"prompt\": prompt_tokens,\n",
    "        }.items():\n",
    "            for type_key, tokens in toks.items():\n",
    "                if type_key == \"token_type_ids\":\n",
    "                    continue\n",
    "                batch[f\"{k}_{type_key}\"] = tokens\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def collate(self, batch):\n",
    "        # first, pad everything to the same length\n",
    "        padded_batch = {}\n",
    "        for k in batch[0].keys():\n",
    "            if k.endswith(\"_input_ids\") or k.endswith(\"_attention_mask\") or k.endswith(\"_labels\"):\n",
    "                # adapted from https://stackoverflow.com/questions/73256206\n",
    "                if \"prompt\" in k:\n",
    "                    to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]\n",
    "                else:\n",
    "                    to_pad = [torch.LongTensor(ex[k]) for ex in batch]\n",
    "                if k.endswith(\"_input_ids\"):\n",
    "                    padding_value = self.tokenizer.pad_token_id\n",
    "                elif k.endswith(\"_labels\"):\n",
    "                    padding_value = self.label_pad_token_id\n",
    "                elif k.endswith(\"_attention_mask\"):\n",
    "                    padding_value = self.padding_value\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected key in batch '{k}'\")\n",
    "\n",
    "                padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n",
    "                # for the prompt, flip back so padding is on left side\n",
    "                if \"prompt\" in k:\n",
    "                    padded_batch[k] = padded_batch[k].flip(dims=[1])\n",
    "            else:\n",
    "                padded_batch[k] = [ex[k] for ex in batch]\n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        tokenized_batch = []\n",
    "\n",
    "        for feature in features:\n",
    "            prompt = feature[\"prompt\"]\n",
    "            chosen = feature[\"chosen\"]\n",
    "            rejected = feature[\"rejected\"]\n",
    "\n",
    "            batch_element = self.tokenize_batch_element(prompt, chosen, rejected)\n",
    "            tokenized_batch.append(batch_element)\n",
    "\n",
    "        # return collated batch\n",
    "        return self.collate(tokenized_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e10bfc9-1a4c-49d0-8bb9-a6a52db610f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global eval_output_record\n",
    "eval_output_record = {}\n",
    "\n",
    "class DPOTrainer(Trainer):\n",
    "    r\"\"\"\n",
    "    Initialize DPOTrainer.\n",
    "\n",
    "    Args:\n",
    "        model (`transformers.PreTrainedModel`):\n",
    "            The model to train, preferably an `AutoModelForSequenceClassification`.\n",
    "        ref_model (`PreTrainedModelWrapper`):\n",
    "            Hugging Face transformer model with a casual language modelling head. Used for implicit reward computation and loss.\n",
    "        beta (`float`, defaults to 0.1):\n",
    "            The beta factor in DPO loss. Higher beta means less divergence from the initial policy.\n",
    "        alpha1 (`float`, defaults to 0):\n",
    "            The alpha factor in Edit-DPO loss (alpha1 * chosen-SFT-loss + alpha2 * DPO-loss).\n",
    "        alpha2 (`float`, defaults to 1):\n",
    "            The alpha factor in Edit-DPO loss (alpha1 * chosen-SFT-loss + alpha2 * DPO-loss).\n",
    "        omega1 (`float`, defaults to 1):\n",
    "            The omeg1a factor in SALT loss: -self.omega1 * policy_chosen_salt_logps - self.omega2 * policy_rejected_salt_logps\n",
    "        omega2 (`float`, defaults to 1):\n",
    "            The omega2 factor in SALT loss: -self.omega1 * policy_chosen_salt_logps - self.omega2 * policy_rejected_salt_logps\n",
    "        args (`transformers.TrainingArguments`):\n",
    "            The arguments to use for training.\n",
    "        data_collator (`transformers.DataCollator`):\n",
    "            The data collator to use for training. If None is specified, the default data collator (`DPODataCollatorWithPadding`) will be used\n",
    "            which will pad the sequences to the maximum length of the sequences in the batch, given a dataset of paired sequences.\n",
    "        label_pad_token_id (`int`, defaults to `-100`):\n",
    "            The label pad token id. This argument is required if you want to use the default data collator.\n",
    "        padding_value (`int`, defaults to `0`):\n",
    "            The padding value. This argument is required if you want to use the default data collator.\n",
    "        truncation_mode (`str`, defaults to `keep_end`):\n",
    "            The truncation mode to use, either `keep_end` or `keep_start`. This argument is required if you want to use the default data collator.\n",
    "        train_dataset (`datasets.Dataset`):\n",
    "            The dataset to use for training.\n",
    "        eval_dataset (`datasets.Dataset`):\n",
    "            The dataset to use for evaluation.\n",
    "        tokenizer (`transformers.PreTrainedTokenizerBase`):\n",
    "            The tokenizer to use for training. This argument is required if you want to use the default data collator.\n",
    "        model_init (`Callable[[], transformers.PreTrainedModel]`):\n",
    "            The model initializer to use for training. If None is specified, the default model initializer will be used.\n",
    "        callbacks (`List[transformers.TrainerCallback]`):\n",
    "            The callbacks to use for training.\n",
    "        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):\n",
    "            The optimizer and scheduler to use for training.\n",
    "        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):\n",
    "            The function to use to preprocess the logits before computing the metrics.\n",
    "        max_length (`int`, defaults to `None`):\n",
    "            The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator.\n",
    "        max_prompt_length (`int`, defaults to `None`):\n",
    "            The maximum length of the prompt. This argument is required if you want to use the default data collator.\n",
    "        peft_config (`Dict`, defaults to `None`):\n",
    "            The PEFT configuration to use for training. If you pass a PEFT configuration, the model will be wrapped in a PEFT model.\n",
    "        alignment_function (`str`, defaults to `None`):\n",
    "            which alignment_function will be used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module] = None,\n",
    "        ref_model: Union[PreTrainedModel, nn.Module] = None,\n",
    "        beta: float = 0.1,\n",
    "        alpha1: float = 0.0,\n",
    "        alpha2: float = 1.0,\n",
    "        omega1: float = 1.0,\n",
    "        omega2: float = 1.0,\n",
    "        S_generated_C_weight: float = 1.0,\n",
    "        S_generated_D_weight: float = -0.1,\n",
    "        S_generated_S_weight: float = -0.1,\n",
    "        S_edited_C_weight: float = 1.0,\n",
    "        S_edited_I_weight: float = 1.0,\n",
    "        S_edited_S_weight: float = 1.0,\n",
    "        output_dir: str = './',\n",
    "        args: TrainingArguments = None,\n",
    "        data_collator: Optional[DataCollator] = None,\n",
    "        label_pad_token_id: int = -100,\n",
    "        padding_value: int = 0,\n",
    "        truncation_mode: str = \"keep_end\",\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
    "        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n",
    "        callbacks: Optional[List[TrainerCallback]] = None,\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\n",
    "            None,\n",
    "            None,\n",
    "        ),\n",
    "        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        max_prompt_length: Optional[int] = None,\n",
    "        peft_config: Optional[Dict] = None,\n",
    "        alignment_function: Optional[str] = None,\n",
    "    ):\n",
    "        if not is_peft_available() and peft_config is not None:\n",
    "            raise ValueError(\n",
    "                \"PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models\"\n",
    "            )\n",
    "        elif is_peft_available() and peft_config is not None:\n",
    "            if getattr(model, \"is_loaded_in_8bit\", False) or getattr(model, \"is_loaded_in_4bit\", False):\n",
    "                model = prepare_model_for_int8_training(model)\n",
    "                self.ref_model = prepare_model_for_int8_training(ref_model)\n",
    "            else:\n",
    "                self.ref_model = ref_model\n",
    "            model = get_peft_model(model, peft_config)\n",
    "            self.ref_model = get_peft_model(self.ref_model, peft_config)\n",
    "            model.print_trainable_parameters()\n",
    "        else:\n",
    "            self.ref_model = ref_model\n",
    "\n",
    "        if data_collator is None:\n",
    "            if tokenizer is None:\n",
    "                raise ValueError(\n",
    "                    \"max_length or a tokenizer must be specified when using the default DPODataCollatorWithPadding\"\n",
    "                )\n",
    "            if max_length is None:\n",
    "                warnings.warn(\n",
    "                    \"When using DPODataCollatorWithPadding, you should set `max_length` in the DPOTrainer's init\"\n",
    "                    \" it will be set to `512` by default, but you should do it yourself in the future.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                max_length = 512\n",
    "            if max_prompt_length is None:\n",
    "                warnings.warn(\n",
    "                    \"When using DPODataCollatorWithPadding, you should set `max_prompt_length` in the DPOTrainer's init\"\n",
    "                    \" it will be set to `128` by default, but you should do it yourself in the future.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                max_prompt_length = 128\n",
    "\n",
    "            data_collator = DPODataCollatorWithPadding(\n",
    "                tokenizer,\n",
    "                max_length=max_length,\n",
    "                max_prompt_length=max_prompt_length,\n",
    "                label_pad_token_id=label_pad_token_id,\n",
    "                padding_value=padding_value,\n",
    "                truncation_mode=truncation_mode,\n",
    "            )\n",
    "\n",
    "            if args.remove_unused_columns:\n",
    "                args.remove_unused_columns = False\n",
    "                # warn users\n",
    "                warnings.warn(\n",
    "                    \"When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments\"\n",
    "                    \" we have set it for you, but you should do it yourself in the future.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "\n",
    "            self.use_dpo_data_collator = True\n",
    "        else:\n",
    "            self.use_dpo_data_collator = False\n",
    "\n",
    "        self.label_pad_token_id = label_pad_token_id\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.beta = beta\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.omega1 = omega1\n",
    "        self.omega2 = omega2\n",
    "\n",
    "        self.S_generated_C_weight = S_generated_C_weight\n",
    "        self.S_generated_D_weight = S_generated_D_weight\n",
    "        self.S_generated_S_weight = S_generated_S_weight\n",
    "        self.S_edited_C_weight = S_edited_C_weight\n",
    "        self.S_edited_I_weight = S_edited_I_weight\n",
    "        self.S_edited_S_weight = S_edited_S_weight\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if alignment_function is None:\n",
    "            self.alignment_function = 'dpo'\n",
    "        else:\n",
    "            self.alignment_function = alignment_function\n",
    "\n",
    "        self.generate_max_length=max_length\n",
    "\n",
    "        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        super().__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            None,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "            preprocess_logits_for_metrics,\n",
    "        )\n",
    "\n",
    "        # Since we inherit from trainer we always have access to an accelerator\n",
    "        if hasattr(self, \"accelerator\"):\n",
    "            model = self.accelerator.prepare_model(model, evaluation_mode=True)\n",
    "            self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Your `Trainer` does not have an `accelerator` object. Consider upgrading `transformers`.\"\n",
    "            )\n",
    "\n",
    "    def sequence_alignment(self, batch: Dict[str, Union[List, torch.LongTensor]]) -> Dict[str, torch.LongTensor]:\n",
    "        rejected_salt_weight = []\n",
    "        chosen_salt_weight = []\n",
    "        \n",
    "        chosen_align_mask = batch['chosen_labels'] != -100\n",
    "        chosen_input_ids = batch['chosen_input_ids'] * chosen_align_mask\n",
    "        \n",
    "        rejected_align_mask = batch['rejected_labels'] != -100\n",
    "        rejected_input_ids = batch['rejected_input_ids'] * rejected_align_mask\n",
    "        \n",
    "        for S_generated, S_edited in zip(rejected_input_ids.tolist(), chosen_input_ids.tolist()):\n",
    "            # Create the instance\n",
    "            alignment = needle.NeedlemanWunsch(S_generated, S_edited) #needle.NeedlemanWunsch, smith.SmithWaterman\n",
    "            alignment.gap_character = -100\n",
    "            # Make the alignment\n",
    "            alignment.align()\n",
    "            # Get the score\n",
    "            alignment.get_score()\n",
    "            # Get the sequences aligned as lists\n",
    "            al_generated, al_edited = alignment.get_aligned_sequences(\"list_of_int\")\n",
    "            w_generated, w_edited = get_position_status(al_generated, \n",
    "                                                        al_edited,\n",
    "                                                        S_generated_C_weight = self.S_generated_C_weight,\n",
    "                                                        S_generated_D_weight = self.S_generated_D_weight,\n",
    "                                                        S_generated_S_weight = self.S_generated_S_weight,\n",
    "                                                        S_edited_C_weight = self.S_edited_C_weight,\n",
    "                                                        S_edited_I_weight = self.S_edited_I_weight,\n",
    "                                                        S_edited_S_weight = self.S_edited_S_weight)\n",
    "        \n",
    "            rejected_salt_weight.append(w_generated)\n",
    "            chosen_salt_weight.append(w_edited)\n",
    "\n",
    "        batch['rejected_salt_weight'] = torch.tensor(rejected_salt_weight, dtype=torch.float32).to(self.accelerator.device)\n",
    "        batch['chosen_salt_weight'] = torch.tensor(chosen_salt_weight, dtype=torch.float32).to(self.accelerator.device)\n",
    "            \n",
    "        return batch\n",
    "\n",
    "    def concatenated_inputs(self, batch: Dict[str, Union[List, torch.LongTensor]]) -> Dict[str, torch.LongTensor]:\n",
    "        \"\"\"Concatenate the chosen and rejected inputs into a single tensor.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of data. Must contain the keys 'chosen_input_ids' and 'rejected_input_ids', which are tensors of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the concatenated inputs under the key 'concatenated_input_ids'.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.alignment_function == 'salt':\n",
    "            batch = self.sequence_alignment(batch)\n",
    "        \n",
    "        max_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n",
    "        concatenated_batch = {}\n",
    "        for k in batch:\n",
    "            if k.startswith(\"chosen\") and isinstance(batch[k], torch.Tensor):\n",
    "                pad_value = self.label_pad_token_id if \"labels\" in k else self.padding_value\n",
    "                concatenated_key = k.replace(\"chosen\", \"concatenated\")\n",
    "                concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)\n",
    "        for k in batch:\n",
    "            if k.startswith(\"rejected\") and isinstance(batch[k], torch.Tensor):\n",
    "                pad_value = self.label_pad_token_id if \"labels\" in k else self.padding_value\n",
    "                concatenated_key = k.replace(\"rejected\", \"concatenated\")\n",
    "                concatenated_batch[concatenated_key] = torch.cat(\n",
    "                    (\n",
    "                        concatenated_batch[concatenated_key],\n",
    "                        pad_to_length(batch[k], max_length, pad_value=pad_value),\n",
    "                    ),\n",
    "                    dim=0,\n",
    "                ).to(self.accelerator.device)\n",
    "        \n",
    "        return concatenated_batch\n",
    "\n",
    "    def dpo_loss(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.FloatTensor,\n",
    "        policy_rejected_logps: torch.FloatTensor,\n",
    "        reference_chosen_logps: torch.FloatTensor,\n",
    "        reference_rejected_logps: torch.FloatTensor,\n",
    "        reference_free: bool = False,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "        Args:\n",
    "            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "            beta: Temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n",
    "            reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n",
    "            The losses tensor contains the DPO loss for each example in the batch.\n",
    "            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n",
    "        \"\"\"\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "        if reference_free:\n",
    "            ref_logratios = 0\n",
    "\n",
    "        logits = pi_logratios - ref_logratios\n",
    "\n",
    "        dpo_losses = -F.logsigmoid(self.beta * logits)\n",
    "        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "        rewards_mask = pi_logratios != 0\n",
    "\n",
    "        return dpo_losses, chosen_rewards, rejected_rewards, rewards_mask\n",
    "\n",
    "    def _get_batch_logps(\n",
    "        self,\n",
    "        logits: torch.FloatTensor,\n",
    "        labels: torch.LongTensor,\n",
    "        average_log_prob: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"Compute the log probabilities of the given labels under the given logits.\n",
    "\n",
    "        Args:\n",
    "            logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n",
    "            labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)\n",
    "            average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.\n",
    "        \"\"\"\n",
    "        if logits.shape[:-1] != labels.shape:\n",
    "            raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "        labels = labels[:, 1:].clone()\n",
    "        logits = logits[:, :-1, :]\n",
    "        loss_mask = labels != self.label_pad_token_id\n",
    "\n",
    "        # dummy token; we'll ignore the losses on these tokens later\n",
    "        labels[labels == self.label_pad_token_id] = 0\n",
    "\n",
    "        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        if average_log_prob:\n",
    "            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "        else:\n",
    "            return (per_token_logps * loss_mask).sum(-1)\n",
    "\n",
    "    def _get_batch_chosen_salt_logps(\n",
    "        self,\n",
    "        logits: torch.FloatTensor,\n",
    "        labels: torch.LongTensor,\n",
    "        labels_weight: torch.LongTensor,\n",
    "        average_log_prob: bool = True,\n",
    "    ) -> torch.FloatTensor:\n",
    "        if logits.shape[:-1] != labels.shape:\n",
    "            raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "        labels = labels[:, 1:].clone()\n",
    "        labels_weight = labels_weight[:, 1:].clone()\n",
    "        logits = logits[:, :-1, :]\n",
    "        loss_mask = labels != self.label_pad_token_id\n",
    "        # dummy token; we'll ignore the losses on these tokens later\n",
    "        labels[labels == self.label_pad_token_id] = 0\n",
    "\n",
    "        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        zero_for_here = torch.tensor(0, dtype=labels_weight.dtype, device=labels_weight.device)\n",
    "        likelihood_weight = torch.where(labels_weight > zero_for_here, labels_weight, zero_for_here)\n",
    "        \n",
    "        likelihood_weight = likelihood_weight*loss_mask\n",
    "        per_token_logps = per_token_logps*likelihood_weight\n",
    "        \n",
    "        if average_log_prob:\n",
    "            likelihood_token_num = (likelihood_weight!=0).sum()\n",
    "            if likelihood_token_num == 0:\n",
    "                return torch.tensor(0, device=logits.device)\n",
    "            else:\n",
    "                return (per_token_logps.sum(-1) / likelihood_token_num).mean()\n",
    "        else:\n",
    "            return per_token_logps.sum(-1).mean()\n",
    "\n",
    "\n",
    "    def _get_batch_rejected_salt_logps(\n",
    "        self,\n",
    "        logits: torch.FloatTensor,\n",
    "        labels: torch.LongTensor,\n",
    "        labels_weight: torch.LongTensor,\n",
    "        average_log_prob: bool = True,\n",
    "        calculate_liklihood_token_log_prob: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "        if logits.shape[:-1] != labels.shape:\n",
    "            raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "        labels = labels[:, 1:].clone()\n",
    "        labels_weight = labels_weight[:, 1:].clone()\n",
    "        logits = logits[:, :-1, :]\n",
    "        loss_mask = labels != self.label_pad_token_id\n",
    "        # dummy token; we'll ignore the losses on these tokens later\n",
    "        labels[labels == self.label_pad_token_id] = 0\n",
    "\n",
    "        # likelihood loss for rejected\n",
    "        if calculate_liklihood_token_log_prob:\n",
    "            likelihood_per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "            zero_for_here = torch.tensor(0, dtype=labels_weight.dtype, device=labels_weight.device)\n",
    "            likelihood_weight = torch.where(labels_weight > zero_for_here, labels_weight, zero_for_here)\n",
    "            \n",
    "            likelihood_weight = likelihood_weight*loss_mask\n",
    "            likelihood_per_token_logps = likelihood_per_token_logps*likelihood_weight\n",
    "            \n",
    "            if average_log_prob:\n",
    "                likelihood_token_num = (likelihood_weight!=0).sum()\n",
    "                if likelihood_token_num == 0:\n",
    "                    likelihood_per_token_logps = torch.tensor(0, device=logits.device)\n",
    "                else:\n",
    "                    likelihood_per_token_logps = (likelihood_per_token_logps.sum(-1) / likelihood_token_num).mean()\n",
    "            else:\n",
    "                likelihood_per_token_logps = likelihood_per_token_logps.sum(-1).mean()\n",
    "\n",
    "        # unlikelihood loss for rejected\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        #torch.log1p(x) := log(x+1)\n",
    "        #log_one_minus_probs = torch.log1p(-probs)\n",
    "        one_minus_probs = 1.0 - probs\n",
    "        one_minus_probs = one_minus_probs + (one_minus_probs==0).float() * 1e-8\n",
    "        log_one_minus_probs = torch.log(one_minus_probs)\n",
    "        unlikelihood_per_token_logps = torch.gather(log_one_minus_probs.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        zero_for_here = torch.tensor(0, dtype=labels_weight.dtype, device=labels_weight.device)\n",
    "        unlikelihood_weight = torch.where(labels_weight < zero_for_here, -1 * labels_weight, zero_for_here)\n",
    "        \n",
    "        unlikelihood_weight = unlikelihood_weight*loss_mask\n",
    "        unlikelihood_per_token_logps = unlikelihood_per_token_logps*unlikelihood_weight\n",
    "\n",
    "        if average_log_prob:\n",
    "            unlikelihood_token_num = (unlikelihood_weight!=0).sum()\n",
    "            if unlikelihood_token_num == 0:\n",
    "                unlikelihood_per_token_logps = torch.tensor(0.0, device=logits.device)\n",
    "            else:\n",
    "                unlikelihood_per_token_logps = (unlikelihood_per_token_logps.sum(-1) / unlikelihood_token_num).mean()\n",
    "        else:\n",
    "            unlikelihood_per_token_logps = unlikelihood_per_token_logps.sum(-1).mean()\n",
    "\n",
    "        if calculate_liklihood_token_log_prob:\n",
    "            return likelihood_per_token_logps + unlikelihood_per_token_logps\n",
    "        else:\n",
    "            return unlikelihood_per_token_logps\n",
    "\n",
    "    def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n",
    "\n",
    "        We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "        \"\"\"\n",
    "        concatenated_batch = self.concatenated_inputs(batch)\n",
    "        all_logits = model(\n",
    "            concatenated_batch[\"concatenated_input_ids\"],\n",
    "            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "        ).logits.to(torch.float32)\n",
    "        \n",
    "        all_logps = self._get_batch_logps(\n",
    "            all_logits,\n",
    "            concatenated_batch[\"concatenated_labels\"],\n",
    "            average_log_prob=True,\n",
    "        )\n",
    "        chosen_logps = all_logps[: batch[\"chosen_input_ids\"].shape[0]]\n",
    "        rejected_logps = all_logps[batch[\"chosen_input_ids\"].shape[0] :]\n",
    "\n",
    "        chosen_logits = all_logits[: batch[\"chosen_input_ids\"].shape[0]]\n",
    "        rejected_logits = all_logits[batch[\"chosen_input_ids\"].shape[0] :]\n",
    "        \n",
    "        if self.alignment_function in ['salt']:\n",
    "            chosen_salt_logps = self._get_batch_chosen_salt_logps(\n",
    "                chosen_logits,\n",
    "                concatenated_batch[\"concatenated_labels\"][: batch[\"chosen_input_ids\"].shape[0]],\n",
    "                concatenated_batch[\"concatenated_salt_weight\"][: batch[\"chosen_input_ids\"].shape[0]],\n",
    "                average_log_prob=True,\n",
    "            )\n",
    "            rejected_salt_logps = self._get_batch_rejected_salt_logps(\n",
    "                rejected_logits,\n",
    "                concatenated_batch[\"concatenated_labels\"][batch[\"chosen_input_ids\"].shape[0] :],\n",
    "                concatenated_batch[\"concatenated_salt_weight\"][batch[\"chosen_input_ids\"].shape[0] :],\n",
    "                average_log_prob=True,\n",
    "                calculate_liklihood_token_log_prob=False\n",
    "            )\n",
    "            return (chosen_logps, rejected_logps, chosen_logits, rejected_logits, chosen_salt_logps, rejected_salt_logps)\n",
    "        else:\n",
    "            return (chosen_logps, rejected_logps, chosen_logits, rejected_logits, None, None)\n",
    "\n",
    "    def get_batch_metrics(\n",
    "        self,\n",
    "        model,\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n",
    "    ):\n",
    "        \"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        (\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            policy_chosen_logits,\n",
    "            policy_rejected_logits,\n",
    "            policy_chosen_salt_logps,\n",
    "            policy_rejected_salt_logps\n",
    "        ) = self.concatenated_forward(model, batch)\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                reference_chosen_logps,\n",
    "                reference_rejected_logps,\n",
    "                _,\n",
    "                _,\n",
    "                _,\n",
    "                _,\n",
    "            ) = self.concatenated_forward(self.ref_model, batch)\n",
    "\n",
    "        dpo_losses, chosen_rewards, rejected_rewards, rewards_mask = self.dpo_loss(\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps,\n",
    "        )\n",
    "        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n",
    "\n",
    "        if self.alignment_function == 'dpo':\n",
    "            losses = (-1) * self.alpha1 * policy_chosen_logps + self.alpha2 * dpo_losses\n",
    "            losses = losses.mean()\n",
    "        elif self.alignment_function == 'sft':\n",
    "            losses = (-1) * policy_chosen_logps\n",
    "            losses = losses.mean()\n",
    "        elif self.alignment_function == 'salt':\n",
    "            losses = (-1) * self.omega1 * policy_chosen_salt_logps + (-1) * self.omega2 * policy_rejected_salt_logps\n",
    "\n",
    "        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n",
    "        metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards[rewards_mask].cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards[rewards_mask].cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies[rewards_mask].cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards)[rewards_mask].cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logps/rejected\"] = policy_rejected_logps[rewards_mask].detach().cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logps/chosen\"] = policy_chosen_logps[rewards_mask].detach().cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logits/rejected\"] = policy_rejected_logits[rewards_mask].detach().cpu().numpy().mean()\n",
    "        metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits[rewards_mask].detach().cpu().numpy().mean()\n",
    "\n",
    "        record_df = None\n",
    "        \n",
    "        if train_eval == \"eval\":\n",
    "            policy_output_decoded, reference_output_decoded = self.get_batch_samples(model, batch)\n",
    "\n",
    "            # Create a DataFrame from the dictionary\n",
    "            batch.update({\n",
    "               'policy_output': policy_output_decoded, \n",
    "               'reference_output': reference_output_decoded \n",
    "            })\n",
    "            record_df = pd.DataFrame({\n",
    "                'Conversation snippet': batch['prompt'], \n",
    "                'S_E': batch['chosen_response_only'], \n",
    "                'S_AI': batch['rejected_response_only'], \n",
    "                'policy_output': batch['policy_output'], \n",
    "                'reference_output': batch['reference_output']\n",
    "            })\n",
    "\n",
    "            global eval_output_record\n",
    "\n",
    "            if self.state.epoch is None:\n",
    "                curr_epoch = '0'\n",
    "            else:\n",
    "                curr_epoch = str(int(self.state.epoch))\n",
    "            \n",
    "            if 'epoch_'+curr_epoch not in eval_output_record.keys():\n",
    "                eval_output_record['epoch_'+curr_epoch] = record_df\n",
    "            else:\n",
    "                eval_output_record['epoch_'+curr_epoch] = pd.concat([eval_output_record['epoch_'+curr_epoch], record_df], ignore_index=True)\n",
    "    \n",
    "            # eval generated summary for policy\n",
    "            eval_dict = ngram_eval.run_all_evaluation(batch['chosen_response_only'], policy_output_decoded)\n",
    "            # UMLS_dict = factev.run_source_concept_faithfulness(ref_sums = batch['chosen_response_only'], \n",
    "                                                               # gen_sums = policy_output_decoded)\n",
    "            # del UMLS_dict['pred_concepts_term']\n",
    "            # del UMLS_dict['pred_concepts_cuis']\n",
    "            # eval_dict.update(UMLS_dict)\n",
    "            eval_dict = {'eval_metrics_policy_'+k: round(v, 4) for k, v in eval_dict.items()}\n",
    "    \n",
    "            # eval generated summary for ref model\n",
    "            ref_eval_dict = ngram_eval.run_all_evaluation(batch['chosen_response_only'], reference_output_decoded)\n",
    "            # ref_UMLS_dict = factev.run_source_concept_faithfulness(ref_sums = batch['chosen_response_only'], \n",
    "            #                                                    gen_sums = reference_output_decoded)\n",
    "            # del ref_UMLS_dict['pred_concepts_term']\n",
    "            # del ref_UMLS_dict['pred_concepts_cuis']\n",
    "            # ref_eval_dict.update(ref_UMLS_dict)\n",
    "            ref_eval_dict = {'eval_metrics_ref_'+k: round(v, 4) for k, v in ref_eval_dict.items()}\n",
    "    \n",
    "            eval_dict.update(ref_eval_dict)\n",
    "            metrics.update(eval_dict)\n",
    "\n",
    "        return losses, metrics\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module],\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        return_outputs=False,\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:\n",
    "        if not self.use_dpo_data_collator:\n",
    "            warnings.warn(\n",
    "                \"compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than \"\n",
    "                \"DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator\"\n",
    "            )\n",
    "        loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"train\")\n",
    "\n",
    "        # force log the metrics\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.store_metrics(metrics, train_eval=\"train\")\n",
    "\n",
    "        if return_outputs:\n",
    "            return (loss, metrics)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def get_batch_samples(self, model, batch: Dict[str, torch.LongTensor]) -> Tuple[str, str]:\n",
    "        \"\"\"Generate samples from the model and reference model for the given batch of inputs.\"\"\"\n",
    "\n",
    "        policy_output = model.generate(\n",
    "            input_ids=batch[\"prompt_input_ids\"],\n",
    "            attention_mask=batch[\"prompt_attention_mask\"],\n",
    "            max_length=self.generate_max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        reference_output = self.ref_model.generate(\n",
    "            input_ids=batch[\"prompt_input_ids\"],\n",
    "            attention_mask=batch[\"prompt_attention_mask\"],\n",
    "            max_length=self.generate_max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        policy_output = pad_to_length(policy_output, self.generate_max_length, self.tokenizer.pad_token_id)\n",
    "        reference_output = pad_to_length(reference_output, self.generate_max_length, self.tokenizer.pad_token_id)\n",
    "\n",
    "        policy_output = policy_output[:, batch[\"prompt_input_ids\"].shape[-1]:]\n",
    "        reference_output = reference_output[:, batch[\"prompt_input_ids\"].shape[-1]:]\n",
    "\n",
    "        policy_output_decoded = self.tokenizer.batch_decode(policy_output, skip_special_tokens=True)\n",
    "        reference_output_decoded = self.tokenizer.batch_decode(reference_output, skip_special_tokens=True)\n",
    "\n",
    "        return policy_output_decoded, reference_output_decoded\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module],\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        if not self.use_dpo_data_collator:\n",
    "            warnings.warn(\n",
    "                \"prediction_step is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than \"\n",
    "                \"DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator\"\n",
    "            )\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(model, \"config\"):\n",
    "                ignore_keys = getattr(model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"eval\")\n",
    "\n",
    "        # force log the metrics\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.store_metrics(metrics, train_eval=\"eval\")\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss.detach(), None, None)\n",
    "\n",
    "        # logits for the chosen and rejected samples from model\n",
    "        logits_dict = {\n",
    "            \"logits_test/chosen\": metrics[\"logits_test/chosen\"],\n",
    "            \"logits_test/rejected\": metrics[\"logits_test/rejected\"],\n",
    "        }\n",
    "        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)\n",
    "        logits = torch.stack(logits).mean(axis=1)\n",
    "        labels = torch.zeros(logits.shape[0])\n",
    "\n",
    "        return (loss.detach(), logits, labels)\n",
    "\n",
    "    def store_metrics(self, metrics: Dict[str, float], train_eval: Literal[\"train\", \"eval\"] = \"train\") -> None:\n",
    "        for key, value in metrics.items():\n",
    "            self._stored_metrics[train_eval][key].append(value)\n",
    "\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training, including stored metrics.\n",
    "\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        # logs either has 'loss' or 'eval_loss'\n",
    "        train_eval = \"train\" if \"loss\" in logs else \"eval\"\n",
    "        # Add averaged stored metrics to logs\n",
    "        for key, metrics in self._stored_metrics[train_eval].items():\n",
    "            logs[key] = torch.tensor(metrics).mean().item()\n",
    "        del self._stored_metrics[train_eval]\n",
    "        return super().log(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f667c645-ee9f-4741-bedd-613813ba6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The arguments for the DPO training script.\n",
    "    \"\"\"\n",
    "\n",
    "    # data parameters\n",
    "    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n",
    "    alpha1: Optional[float] = field(default=0.0, metadata={\"help\": \"the alpha parameter for Edit-DPO loss\"})\n",
    "    alpha2: Optional[float] = field(default=1.0, metadata={\"help\": \"the alpha parameter for Edit-DPO loss\"})\n",
    "    omega1: Optional[float] = field(default=1.0, metadata={\"help\": \"the omega parameter for SALT loss\"})\n",
    "    omega2: Optional[float] = field(default=1.0, metadata={\"help\": \"the omega parameter for SALT loss\"})\n",
    "    S_generated_C_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_generated_D_weight: Optional[float] = field(default=-0.1, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_generated_S_weight: Optional[float] = field(default=-0.1, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_edited_C_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_edited_I_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_edited_S_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "\n",
    "    # training parameters\n",
    "    model_name_or_path: Optional[str] = field(default=\"gpt2\", metadata={\"help\": \"the model name\"})\n",
    "    learning_rate: Optional[float] = field(default=1e-3, metadata={\"help\": \"optimizer learning rate\"})\n",
    "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"batch size per device\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"batch size per device\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    max_length: Optional[int] = field(default=512, metadata={\"help\": \"max length of each sample\"})\n",
    "    max_prompt_length: Optional[int] = field(default=128, metadata={\"help\": \"max length of each sample's prompt\"})\n",
    "    label_pad_token_id: Optional[int] = field(default=-100, metadata={\"help\": \"label for non response tokens\"})\n",
    "    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"max number of training steps\"})\n",
    "    num_train_epochs: Optional[int] = field(default=1, metadata={\"help\": \"the number of training epochs\"})\n",
    "    evaluation_strategy: Optional[str] = field(default=None, metadata={\"help\": \"the evaluation strategy, None, epoch, or steps\"})\n",
    "    eval_steps: Optional[int] = field(default=500, metadata={\"help\": \"Number of update steps between two evaluations if evaluation_strategy=steps\"})\n",
    "    eval_first_step: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to eval first step\"})\n",
    "    logging_strategy: Optional[str] = field(default=None, metadata={\"help\": \"the logging strategy, None, epoch, or steps\"})\n",
    "    log_steps: Optional[int] = field(default=500, metadata={\"help\": \"Number of update steps between two logging if logging_strategy=steps\"})\n",
    "    logging_first_step: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to log first step\"})\n",
    "    save_strategy: Optional[str] = field(default=None, metadata={\"help\": \"the saving strategy, None, epoch, or steps\"})\n",
    "    save_steps: Optional[int] = field(default=500, metadata={\"help\": \"Number of update steps between two saving if save_strategy=steps\"})\n",
    "    load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})\n",
    "    use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n",
    "    alignment_function: Optional[str] = field(default='dpo', metadata={\"help\": \"alignment function will be used\"})\n",
    "    output_dir: Optional[str] = field(default='./test', metadata={\"help\": \"output path\"})\n",
    "    run_name: Optional[str] = field(default='test', metadata={\"help\": \"A descriptor for the run. Typically used for wandb and mlflow logging.\"})\n",
    "    save_total_limit: Optional[int] = field(default=1, metadata={\"help\": \"If a value is passed, will limit the total amount of checkpoints.\"})\n",
    "    load_best_model_at_end: Optional[bool] = field(default=False, metadata={\"help\": \"Whether or not to load the best model found during training at the end of training.\"})\n",
    "    metric_for_best_model: Optional[str] = field(default=None, metadata={\"help\": \"Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models.\"})\n",
    "    # instrumentation\n",
    "    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n",
    "    report_to: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n",
    "            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n",
    "            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n",
    "        },\n",
    "    )\n",
    "    # debug argument for distributed training\n",
    "    ignore_bias_buffers: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n",
    "            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f126a3b-965a-4195-8be7-cd28656c4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(ScriptArguments)\n",
    "\n",
    "# #for SFT\n",
    "# script_args = parser.parse_args_into_dataclasses(args=['--per_device_train_batch_size', '4',\n",
    "#                                                        '--per_device_eval_batch_size', '16',\n",
    "#                                                        '--gradient_accumulation_steps', '2',\n",
    "#                                                        '--model_name_or_path', 'gpt2',\n",
    "#                                                        # '--model_name_or_path', 'huggy llama/llama-7b',\n",
    "#                                                        # '--model_name_or_path', 'meta-llama/Llama-2-7b-hf',\n",
    "#                                                        # '--load_in_4bit',\n",
    "#                                                        # '--use_peft',\n",
    "#                                                        # '--learning_rate', '1e-3',\n",
    "#                                                        '--learning_rate', '1e-4',\n",
    "#                                                        '--report_to', 'wandb',\n",
    "#                                                        '--run_name', 'SFT-avs-gpt2',\n",
    "#                                                        '--max_length', '1024',\n",
    "#                                                        '--max_prompt_length', '768',\n",
    "#                                                        '--num_train_epochs', '20',\n",
    "#                                                        '--max_steps', '-1',\n",
    "#                                                        '--evaluation_strategy', 'epoch',\n",
    "#                                                        '--eval_steps', '-1',\n",
    "#                                                        # '--eval_first_step',\n",
    "#                                                        '--logging_strategy', 'steps',\n",
    "#                                                        '--log_steps', '10',\n",
    "#                                                        '--logging_first_step',\n",
    "#                                                        '--save_strategy', 'epoch',\n",
    "#                                                        '--save_steps', '-1',\n",
    "#                                                        '--save_total_limit', '3',\n",
    "#                                                        '--load_best_model_at_end',\n",
    "#                                                        '--metric_for_best_model', 'metrics_policy_rouge1',\n",
    "#                                                        '--alignment_function', 'sft',\n",
    "#                                                        '--output_dir', './results/avs/SFT_model/gpt2',\n",
    "#                                                        # '--output_dir', './results/SFT_model/llama2_7b',\n",
    "#                                                       ])[0]\n",
    "\n",
    "# #for DPO\n",
    "script_args = parser.parse_args_into_dataclasses(args=['--per_device_train_batch_size', '1',\n",
    "                                                       '--per_device_eval_batch_size', '2',\n",
    "                                                       '--gradient_accumulation_steps', '8',\n",
    "                                                       # '--model_name_or_path', 'results/avs/BASE_model/gpt2',\n",
    "                                                       '--model_name_or_path', 'gpt2',\n",
    "                                                       # '--model_name_or_path', 'meta-llama/Llama-2-7b-hf',\n",
    "                                                       # '--load_in_4bit',\n",
    "                                                       # '--use_peft',\n",
    "                                                       # '--learning_rate', '1e-3',\n",
    "                                                       '--learning_rate', '1e-4',\n",
    "                                                       '--report_to', 'wandb',\n",
    "                                                       '--run_name', 'DPO-avs-gpt2',\n",
    "                                                       '--max_length', '1024',\n",
    "                                                       '--max_prompt_length', '768',\n",
    "                                                       '--num_train_epochs', '5',\n",
    "                                                       '--max_steps', '-1',\n",
    "                                                       '--evaluation_strategy', 'epoch',\n",
    "                                                       '--eval_steps', '-1',\n",
    "                                                       # '--eval_first_step',\n",
    "                                                       '--logging_strategy', 'steps',\n",
    "                                                       '--log_steps', '20',\n",
    "                                                       '--logging_first_step',\n",
    "                                                       # '--save_strategy', 'epoch',\n",
    "                                                       '--save_strategy', 'steps',\n",
    "                                                       '--save_steps', '10000000',\n",
    "                                                       # '--save_total_limit', '3',\n",
    "                                                       # '--load_best_model_at_end',\n",
    "                                                       # '--metric_for_best_model', 'metrics_policy_rouge1',\n",
    "                                                       '--alignment_function', 'dpo',\n",
    "                                                       '--output_dir', './results/avs/DPO_model/DPO-avs-gpt2(1|1|0.3)',\n",
    "                                                       '--alpha1', '1.0', #sft loss\n",
    "                                                       '--alpha2', '1.0', #dpo loss\n",
    "                                                       '--beta', '0.3',\n",
    "                                                      ])[0]\n",
    "\n",
    "# # for SALT\n",
    "# script_args = parser.parse_args_into_dataclasses(args=['--per_device_train_batch_size', '4',\n",
    "#                                                        '--per_device_eval_batch_size', '8',\n",
    "#                                                        '--gradient_accumulation_steps', '2',\n",
    "#                                                        # '--model_name_or_path', 'gpt2',\n",
    "#                                                        # '--model_name_or_path', 'results/avs/BASE_model/gpt2',\n",
    "#                                                        # '--model_name_or_path', 'huggy llama/llama-7b',\n",
    "#                                                        '--model_name_or_path', 'meta-llama/Llama-2-7b-hf',\n",
    "#                                                        '--load_in_4bit',\n",
    "#                                                        '--use_peft',\n",
    "#                                                        '--learning_rate', '1e-3',\n",
    "#                                                        # '--learning_rate', '1e-4',\n",
    "#                                                        '--report_to', 'wandb',\n",
    "#                                                        '--run_name', 'SALT-avs-llama2',\n",
    "#                                                        '--max_length', '1024',\n",
    "#                                                        '--max_prompt_length', '768',\n",
    "#                                                        '--num_train_epochs', '5',\n",
    "#                                                        '--max_steps', '-1',\n",
    "#                                                        '--evaluation_strategy', 'epoch',\n",
    "#                                                        '--eval_steps', '-1',\n",
    "#                                                        # '--eval_first_step',\n",
    "#                                                        '--logging_strategy', 'steps',\n",
    "#                                                        '--log_steps', '10',\n",
    "#                                                        '--logging_first_step',\n",
    "#                                                        # '--save_strategy', 'epoch',\n",
    "#                                                        '--save_strategy', 'steps',\n",
    "#                                                        '--save_steps', '10000000',\n",
    "#                                                        # '--save_total_limit', '3',\n",
    "#                                                        # '--load_best_model_at_end',\n",
    "#                                                        # '--metric_for_best_model', 'metrics_policy_rouge1',\n",
    "#                                                        '--alignment_function', 'salt',\n",
    "#                                                        '--output_dir', './results/avs/SALT_model/SALT-avs-llama2(1|-0.1|-0.1|1|1.1|1.1)',\n",
    "#                                                        '--omega1', '1.0', #salt chosen likelihood loss weight\n",
    "#                                                        '--omega2', '0.1', #salt rejected unlikelihood loss weight\n",
    "#                                                        '--S_generated_C_weight', '1.0', #sequence alignment weights\n",
    "#                                                        '--S_generated_D_weight', '-0.1', #sequence alignment weights\n",
    "#                                                        '--S_generated_S_weight', '-0.1', #sequence alignment weights\n",
    "#                                                        '--S_edited_C_weight', '1.0', #sequence alignment weights\n",
    "#                                                        '--S_edited_I_weight', '1.1', #sequence alignment weights\n",
    "#                                                        '--S_edited_S_weight', '1.1', #sequence alignment weights       \n",
    "#                                                       ])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048f5928-ea73-41c7-9d2b-3102a01ba9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzonghaiyao\u001b[0m (\u001b[33mgraph-to-text-qagnn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/zonghaiyao_umass_edu/SALT/RLHF/wandb/run-20230926_135703-vk5xkbmx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/graph-to-text-qagnn/uncategorized/runs/vk5xkbmx' target=\"_blank\">worldly-violet-34</a></strong> to <a href='https://wandb.ai/graph-to-text-qagnn/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/graph-to-text-qagnn/uncategorized' target=\"_blank\">https://wandb.ai/graph-to-text-qagnn/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/graph-to-text-qagnn/uncategorized/runs/vk5xkbmx' target=\"_blank\">https://wandb.ai/graph-to-text-qagnn/uncategorized/runs/vk5xkbmx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/graph-to-text-qagnn/uncategorized/runs/vk5xkbmx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f89cf3f4dc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ac8c6e-7255-4ba0-ab17-cd4e1c589abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. load a pretrained model\n",
    "if script_args.load_in_8bit and script_args.load_in_4bit:\n",
    "    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "elif script_args.load_in_8bit or script_args.load_in_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit\n",
    "    )\n",
    "    # This means: fit the entire model on the GPU:0\n",
    "    device_map = {\"\": 0}\n",
    "else:\n",
    "    device_map = None\n",
    "    quantization_config = None\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path,\n",
    "                                             use_auth_token = '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',\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             device_map=device_map,\n",
    "                                            )\n",
    "\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "if script_args.ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "# model_ref = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path)\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path,\n",
    "                                                 use_auth_token = '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',\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 device_map=device_map,\n",
    "                                                )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path, use_auth_token = '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')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b9c6fb2-a4a2-40cc-8e57-298a1a68d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt(prompt_and_response):\n",
    "    search_term = \"\\n\\nGenerate the corresponding Discharge Instructions according to the input article:\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "\n",
    "def load_dataset(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"Load the dataset from Hugging Face and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts should be structured as follows:\n",
    "      Conversation <prompt>\\n\\nSummary\n",
    "    \"\"\"\n",
    "    # dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)\n",
    "    # dataset = load_from_disk('data/eden/DPO/' + split)\n",
    "    if script_args.alignment_function in ['sft', 'dpo', 'salt']:\n",
    "        dataset = load_from_disk('data/avs/' + split)\n",
    "        \n",
    "    if sanity_check:\n",
    "        print('only train on 1000 samples')\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample) -> Dict[str, str]:\n",
    "        prompt = extract_prompt(sample[\"chosen\"])\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt) :],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt) :],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses)\n",
    "\n",
    "# 2. Load training dataset\n",
    "train_dataset = load_dataset(\"train\", sanity_check=script_args.sanity_check)\n",
    "\n",
    "# 3. Load evaluation dataset\n",
    "eval_dataset = load_dataset(\"sub_eval\", sanity_check=script_args.sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "010b135d-b20c-4fe8-989a-4a6e8e354d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize training arguments:\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    num_train_epochs=script_args.num_train_epochs,\n",
    "    max_steps=script_args.max_steps,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    evaluation_strategy=script_args.evaluation_strategy,\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    logging_strategy=script_args.logging_strategy,\n",
    "    logging_steps=script_args.log_steps,\n",
    "    logging_first_step=script_args.logging_first_step,\n",
    "    save_strategy=script_args.save_strategy,\n",
    "    save_steps=script_args.save_steps,\n",
    "    output_dir=script_args.output_dir,\n",
    "    report_to=script_args.report_to,\n",
    "    run_name=script_args.run_name,\n",
    "    save_total_limit=script_args.save_total_limit,\n",
    "    load_best_model_at_end=script_args.load_best_model_at_end,\n",
    "    metric_for_best_model=script_args.metric_for_best_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcfd6022-adb3-43fc-8cba-89c246a1999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. initialize the DPO trainer\n",
    "\n",
    "if script_args.use_peft:\n",
    "    lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    lora_config = None\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=script_args.beta,\n",
    "    alpha1=script_args.alpha1,\n",
    "    alpha2=script_args.alpha2,\n",
    "    omega1=script_args.omega1,\n",
    "    omega2=script_args.omega2,\n",
    "    S_generated_C_weight=script_args.S_generated_C_weight,\n",
    "    S_generated_D_weight=script_args.S_generated_D_weight,\n",
    "    S_generated_S_weight=script_args.S_generated_S_weight,\n",
    "    S_edited_C_weight=script_args.S_edited_C_weight,\n",
    "    S_edited_I_weight=script_args.S_edited_I_weight,\n",
    "    S_edited_S_weight=script_args.S_edited_S_weight,\n",
    "    output_dir=script_args.output_dir,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=script_args.max_length,\n",
    "    max_prompt_length=script_args.max_prompt_length,\n",
    "    peft_config=lora_config,\n",
    "    alignment_function=script_args.alignment_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52600907-9df7-4743-b7b3-8a2828ca3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if script_args.eval_first_step:\n",
    "    print(dpo_trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f87460a-ed40-460e-8532-87f94a7321e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.76 GiB total capacity; 8.89 GiB already allocated; 154.69 MiB free; 9.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 6. train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "Cell \u001b[0;32mIn[8], line 638\u001b[0m, in \u001b[0;36mDPOTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# torch.autograd.set_detect_anomaly(True)\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m--> 638\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    641\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 605\u001b[0m, in \u001b[0;36mDPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dpo_data_collator:\n\u001b[1;32m    601\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# force log the metrics\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mis_main_process:\n",
      "Cell \u001b[0;32mIn[8], line 503\u001b[0m, in \u001b[0;36mDPOTrainer.get_batch_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    496\u001b[0m (\n\u001b[1;32m    497\u001b[0m     policy_chosen_logps,\n\u001b[1;32m    498\u001b[0m     policy_rejected_logps,\n\u001b[1;32m    499\u001b[0m     policy_chosen_logits,\n\u001b[1;32m    500\u001b[0m     policy_rejected_logits,\n\u001b[1;32m    501\u001b[0m     policy_chosen_salt_logps,\n\u001b[1;32m    502\u001b[0m     policy_rejected_salt_logps\n\u001b[0;32m--> 503\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenated_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    505\u001b[0m     (\n\u001b[1;32m    506\u001b[0m         reference_chosen_logps,\n\u001b[1;32m    507\u001b[0m         reference_rejected_logps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         _,\n\u001b[1;32m    512\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcatenated_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model, batch)\n",
      "Cell \u001b[0;32mIn[8], line 453\u001b[0m, in \u001b[0;36mDPOTrainer.concatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03mWe do this to avoid doing two forward passes, because it's faster for FSDP.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m concatenated_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcatenated_inputs(batch)\n\u001b[0;32m--> 453\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    458\u001b[0m all_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_batch_logps(\n\u001b[1;32m    459\u001b[0m     all_logits,\n\u001b[1;32m    460\u001b[0m     concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    461\u001b[0m     average_log_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    463\u001b[0m chosen_logps \u001b[38;5;241m=\u001b[39m all_logps[: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/work/zonghaiyao_umass_edu/anaconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:183\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 183\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n\u001b[1;32m    186\u001b[0m         attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m    187\u001b[0m             [], value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    188\u001b[0m         )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 10.76 GiB total capacity; 8.89 GiB already allocated; 154.69 MiB free; 9.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 6. train\n",
    "\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0395494-2760-42a1-86a2-c8dfeae22170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in eval_output_record.items():\n",
    "    df.to_csv(script_args.output_dir + '/' + str(key) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfa700-0e4e-4182-bc17-50a60e3e546c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172922b-e66f-469b-bd44-cd8694782835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salt",
   "language": "python",
   "name": "salt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
