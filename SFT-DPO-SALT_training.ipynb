{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cb4af4-9aef-4783-8765-f47d731cfe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5c0711-89fd-4727-882e-054080cc042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global debug\n",
    "debug = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39055ed0-f1bf-4f59-8e25-77265c96c6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a80afd-66fc-47f0-ad8a-2d290f3c26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. imports\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "# import time\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset,  load_from_disk#, load_dataset, load_metric\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "# from transformers.trainer_utils import EvalPrediction# , EvalLoopOutput\n",
    "# from transformers.trainer_pt_utils import find_batch_size, nested_concat\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "from dpo import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca458788-cebc-44e4-aeb8-6adf4007b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metrics import AutomaticNgramEval, Rouge, AutomaticFactEval\n",
    "\n",
    "# ngram_eval = AutomaticNgramEval()\n",
    "# # factev = AutomaticFactEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e10bfc9-1a4c-49d0-8bb9-a6a52db610f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global eval_output_record\n",
    "# eval_output_record = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6536d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt(prompt_and_response):\n",
    "    search_term = \"\\n\\nGenerate the corresponding Discharge Instructions according to the input article:\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "\n",
    "def load_dataset(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"Load the dataset from Hugging Face and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts should be structured as follows:\n",
    "      Conversation <prompt>\\n\\nSummary\n",
    "    \"\"\"\n",
    "    # dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)\n",
    "    # dataset = load_from_disk('data/eden/DPO/' + split)\n",
    "    if script_args.alignment_function in ['sft', 'dpo', 'salt']:\n",
    "        dataset = load_from_disk('data/avs/' + split)\n",
    "        \n",
    "    if sanity_check:\n",
    "        print('only train on 1000 samples')\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample) -> Dict[str, str]:\n",
    "        prompt = extract_prompt(sample[\"chosen\"])\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt) :],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt) :],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f667c645-ee9f-4741-bedd-613813ba6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and parse arguments.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The arguments for the DPO training script.\n",
    "    \"\"\"\n",
    "\n",
    "    # data parameters\n",
    "    beta: Optional[float] = field(default=0.1, metadata={\"help\": \"the beta parameter for DPO loss\"})\n",
    "    alpha1: Optional[float] = field(default=0.0, metadata={\"help\": \"the alpha parameter for Edit-DPO loss\"})\n",
    "    alpha2: Optional[float] = field(default=1.0, metadata={\"help\": \"the alpha parameter for Edit-DPO loss\"})\n",
    "    omega1: Optional[float] = field(default=1.0, metadata={\"help\": \"the omega parameter for SALT loss\"})\n",
    "    omega2: Optional[float] = field(default=1.0, metadata={\"help\": \"the omega parameter for SALT loss\"})\n",
    "    S_generated_C_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_generated_D_weight: Optional[float] = field(default=-0.1, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_generated_S_weight: Optional[float] = field(default=-0.1, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_edited_C_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_edited_I_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "    S_edited_S_weight: Optional[float] = field(default=1.0, metadata={\"help\": \"sequence alignment weights\"})\n",
    "\n",
    "    # training parameters\n",
    "    model_name_or_path: Optional[str] = field(default=\"gpt2\", metadata={\"help\": \"the model name\"})\n",
    "    learning_rate: Optional[float] = field(default=1e-3, metadata={\"help\": \"optimizer learning rate\"})\n",
    "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"batch size per device\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"batch size per device\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    max_length: Optional[int] = field(default=512, metadata={\"help\": \"max length of each sample\"})\n",
    "    max_prompt_length: Optional[int] = field(default=128, metadata={\"help\": \"max length of each sample's prompt\"})\n",
    "    label_pad_token_id: Optional[int] = field(default=-100, metadata={\"help\": \"label for non response tokens\"})\n",
    "    max_steps: Optional[int] = field(default=1000, metadata={\"help\": \"max number of training steps\"})\n",
    "    num_train_epochs: Optional[int] = field(default=1, metadata={\"help\": \"the number of training epochs\"})\n",
    "    evaluation_strategy: Optional[str] = field(default=None, metadata={\"help\": \"the evaluation strategy, None, epoch, or steps\"})\n",
    "    eval_steps: Optional[int] = field(default=500, metadata={\"help\": \"Number of update steps between two evaluations if evaluation_strategy=steps\"})\n",
    "    eval_first_step: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to eval first step\"})\n",
    "    logging_strategy: Optional[str] = field(default=None, metadata={\"help\": \"the logging strategy, None, epoch, or steps\"})\n",
    "    log_steps: Optional[int] = field(default=500, metadata={\"help\": \"Number of update steps between two logging if logging_strategy=steps\"})\n",
    "    logging_first_step: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to log first step\"})\n",
    "    save_strategy: Optional[str] = field(default=None, metadata={\"help\": \"the saving strategy, None, epoch, or steps\"})\n",
    "    save_steps: Optional[int] = field(default=500, metadata={\"help\": \"Number of update steps between two saving if save_strategy=steps\"})\n",
    "    load_in_8bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 8 bits precision\"})\n",
    "    load_in_4bit: Optional[bool] = field(default=False, metadata={\"help\": \"load the model in 4 bits precision\"})\n",
    "    use_peft: Optional[bool] = field(default=False, metadata={\"help\": \"Wether to use PEFT or not to train adapters\"})\n",
    "    alignment_function: Optional[str] = field(default='dpo', metadata={\"help\": \"alignment function will be used\"})\n",
    "    output_dir: Optional[str] = field(default='./test', metadata={\"help\": \"output path\"})\n",
    "    run_name: Optional[str] = field(default='test', metadata={\"help\": \"A descriptor for the run. Typically used for wandb and mlflow logging.\"})\n",
    "    save_total_limit: Optional[int] = field(default=1, metadata={\"help\": \"If a value is passed, will limit the total amount of checkpoints.\"})\n",
    "    load_best_model_at_end: Optional[bool] = field(default=False, metadata={\"help\": \"Whether or not to load the best model found during training at the end of training.\"})\n",
    "    metric_for_best_model: Optional[str] = field(default=None, metadata={\"help\": \"Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models.\"})\n",
    "    # instrumentation\n",
    "    sanity_check: Optional[bool] = field(default=False, metadata={\"help\": \"only train on 1000 samples\"})\n",
    "    report_to: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": 'The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,'\n",
    "            '`\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. '\n",
    "            'Use `\"all\"` to report to all integrations installed, `\"none\"` for no integrations.'\n",
    "        },\n",
    "    )\n",
    "    # debug argument for distributed training\n",
    "    ignore_bias_buffers: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See\"\n",
    "            \"https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992\"\n",
    "        },\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f126a3b-965a-4195-8be7-cd28656c4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(ScriptArguments)\n",
    "\n",
    "# #for SFT\n",
    "# script_args = parser.parse_args_into_dataclasses(\n",
    "#         args=[\n",
    "#                 '--per_device_train_batch_size', '4',\n",
    "#                 '--per_device_eval_batch_size', '16',\n",
    "#                 '--gradient_accumulation_steps', '2',\n",
    "#                 '--model_name_or_path', 'gpt2',\n",
    "#                 # '--model_name_or_path', 'huggy llama/llama-7b',\n",
    "#                 # '--model_name_or_path', 'meta-llama/Llama-2-7b-hf',\n",
    "#                 # '--load_in_4bit',\n",
    "#                 # '--use_peft',\n",
    "#                 # '--learning_rate', '1e-3',\n",
    "#                 '--learning_rate', '1e-4',\n",
    "#                 # '--report_to', 'wandb',\n",
    "#                 '--run_name', 'SFT-avs-gpt2',\n",
    "#                 '--max_length', '1024',\n",
    "#                 '--max_prompt_length', '768',\n",
    "#                 '--num_train_epochs', '20',\n",
    "#                 '--max_steps', '-1',\n",
    "#                 '--evaluation_strategy', 'epoch',\n",
    "#                 '--eval_steps', '-1',\n",
    "#                 # '--eval_first_step',\n",
    "#                 '--logging_strategy', 'steps',\n",
    "#                 '--log_steps', '10',\n",
    "#                 '--logging_first_step',\n",
    "#                 '--save_strategy', 'epoch',\n",
    "#                 '--save_steps', '-1',\n",
    "#                 '--save_total_limit', '3',\n",
    "#                 '--load_best_model_at_end',\n",
    "#                 '--metric_for_best_model', 'metrics_policy_rouge1',\n",
    "#                 '--alignment_function', 'sft',\n",
    "#                 '--output_dir', './results/avs/SFT_model/gpt2',\n",
    "#                 # '--output_dir', './results/SFT_model/llama2_7b',\n",
    "#             ]\n",
    "#         )[0]\n",
    "\n",
    "# # # #for DPO\n",
    "# script_args = parser.parse_args_into_dataclasses(args=['--per_device_train_batch_size', '1',\n",
    "#                                                        '--per_device_eval_batch_size', '2',\n",
    "#                                                        '--gradient_accumulation_steps', '8',\n",
    "#                                                        # '--model_name_or_path', 'results/avs/BASE_model/gpt2',\n",
    "#                                                        '--model_name_or_path', 'gpt2',\n",
    "#                                                        # '--model_name_or_path', 'meta-llama/Llama-2-7b-hf',\n",
    "#                                                        # '--load_in_4bit',\n",
    "#                                                        # '--use_peft',\n",
    "#                                                        # '--learning_rate', '1e-3',\n",
    "#                                                        '--learning_rate', '1e-4',\n",
    "#                                                        # '--report_to', 'wandb',\n",
    "#                                                        '--run_name', 'DPO-avs-gpt2',\n",
    "#                                                        '--max_length', '1024',\n",
    "#                                                        '--max_prompt_length', '768',\n",
    "#                                                        '--num_train_epochs', '5',\n",
    "#                                                        '--max_steps', '-1',\n",
    "#                                                        '--evaluation_strategy', 'epoch',\n",
    "#                                                        '--eval_steps', '-1',\n",
    "#                                                        # '--eval_first_step',\n",
    "#                                                        '--logging_strategy', 'steps',\n",
    "#                                                        '--log_steps', '20',\n",
    "#                                                        '--logging_first_step',\n",
    "#                                                        # '--save_strategy', 'epoch',\n",
    "#                                                        '--save_strategy', 'steps',\n",
    "#                                                        '--save_steps', '10000000',\n",
    "#                                                        # '--save_total_limit', '3',\n",
    "#                                                        # '--load_best_model_at_end',\n",
    "#                                                        # '--metric_for_best_model', 'metrics_policy_rouge1',\n",
    "#                                                        '--alignment_function', 'dpo',\n",
    "#                                                        '--output_dir', './results/avs/DPO_model/DPO-avs-gpt2(1|1|0.3)',\n",
    "#                                                        '--alpha1', '1.0', #sft loss\n",
    "#                                                        '--alpha2', '1.0', #dpo loss\n",
    "#                                                        '--beta', '0.3',\n",
    "#                                                       ])[0]\n",
    "\n",
    "# # # for SALT\n",
    "script_args = parser.parse_args_into_dataclasses(args=['--per_device_train_batch_size', '4',\n",
    "                                                       '--per_device_eval_batch_size', '8',\n",
    "                                                       '--gradient_accumulation_steps', '2',\n",
    "                                                       # '--model_name_or_path', 'gpt2',\n",
    "                                                       # '--model_name_or_path', 'results/avs/BASE_model/gpt2',\n",
    "                                                       # '--model_name_or_path', 'huggy llama/llama-7b',\n",
    "                                                       '--model_name_or_path', 'meta-llama/Llama-2-7b-hf',\n",
    "                                                       '--load_in_4bit',\n",
    "                                                       '--use_peft',\n",
    "                                                       '--learning_rate', '1e-3',\n",
    "                                                       # '--learning_rate', '1e-4',\n",
    "                                                       '--report_to', 'wandb',\n",
    "                                                       # '--run_name', 'SALT-avs-llama2',\n",
    "                                                       '--max_length', '1024',\n",
    "                                                       '--max_prompt_length', '768',\n",
    "                                                       '--num_train_epochs', '5',\n",
    "                                                       '--max_steps', '-1',\n",
    "                                                       '--evaluation_strategy', 'epoch',\n",
    "                                                       '--eval_steps', '-1',\n",
    "                                                       # '--eval_first_step',\n",
    "                                                       '--logging_strategy', 'steps',\n",
    "                                                       '--log_steps', '10',\n",
    "                                                       '--logging_first_step',\n",
    "                                                       # '--save_strategy', 'epoch',\n",
    "                                                       '--save_strategy', 'steps',\n",
    "                                                       '--save_steps', '10000000',\n",
    "                                                       # '--save_total_limit', '3',\n",
    "                                                       # '--load_best_model_at_end',\n",
    "                                                       # '--metric_for_best_model', 'metrics_policy_rouge1',\n",
    "                                                       '--alignment_function', 'salt',\n",
    "                                                       '--output_dir', './results/avs/SALT_model/SALT-avs-llama2(1|-0.1|-0.1|1|1.1|1.1)',\n",
    "                                                       '--omega1', '1.0', #salt chosen likelihood loss weight\n",
    "                                                       '--omega2', '0.1', #salt rejected unlikelihood loss weight\n",
    "                                                       '--S_generated_C_weight', '1.0', #sequence alignment weights\n",
    "                                                       '--S_generated_D_weight', '-0.1', #sequence alignment weights\n",
    "                                                       '--S_generated_S_weight', '-0.1', #sequence alignment weights\n",
    "                                                       '--S_edited_C_weight', '1.0', #sequence alignment weights\n",
    "                                                       '--S_edited_I_weight', '1.1', #sequence alignment weights\n",
    "                                                       '--S_edited_S_weight', '1.1', #sequence alignment weights       \n",
    "                                                      ])[0]\n",
    "\n",
    "# 2. Load training dataset\n",
    "train_dataset = load_dataset(\"train\", sanity_check=script_args.sanity_check)\n",
    "\n",
    "# 3. Load evaluation dataset\n",
    "eval_dataset = load_dataset(\"sub_eval\", sanity_check=script_args.sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048f5928-ea73-41c7-9d2b-3102a01ba9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fdff723",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hg_secret', 'r') as f:\n",
    "    hg_auth_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ac8c6e-7255-4ba0-ab17-cd4e1c589abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sai.prabhakar/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     device_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     quantization_config \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(script_args\u001b[39m.\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                                              use_auth_token \u001b[39m=\u001b[39;49m hg_auth_token,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                              quantization_config\u001b[39m=\u001b[39;49mquantization_config,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                              device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                             )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# model = get_peft_model(model, lora_config)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# model.print_trainable_parameters()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m script_args\u001b[39m.\u001b[39mignore_bias_buffers:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# torch distributed hack\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/modeling_utils.py:2614\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2614\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2615\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2616\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2617\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m pip install bitsandbytes` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2618\u001b[0m         )\n\u001b[1;32m   2620\u001b[0m     \u001b[39mif\u001b[39;00m torch_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2621\u001b[0m         \u001b[39m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2622\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2623\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOverriding torch_dtype=\u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2624\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2625\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2626\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2627\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "# 1. load a pretrained model\n",
    "if script_args.load_in_8bit and script_args.load_in_4bit:\n",
    "    raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "elif script_args.load_in_8bit or script_args.load_in_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit\n",
    "    )\n",
    "    # This means: fit the entire model on the GPU:0\n",
    "    device_map = {\"\": 0}\n",
    "else:\n",
    "    device_map = None\n",
    "    quantization_config = None\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path,\n",
    "                                             use_auth_token = hg_auth_token,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             device_map=device_map,\n",
    "                                            )\n",
    "\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "if script_args.ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "# model_ref = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path)\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path,\n",
    "                                                 use_auth_token = hg_auth_token,\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 device_map=device_map,\n",
    "                                                )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path, use_auth_token=hg_auth_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9c6fb2-a4a2-40cc-8e57-298a1a68d153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010b135d-b20c-4fe8-989a-4a6e8e354d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize training arguments:\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    num_train_epochs=script_args.num_train_epochs,\n",
    "    max_steps=script_args.max_steps,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    evaluation_strategy=script_args.evaluation_strategy,\n",
    "    eval_steps=script_args.eval_steps,\n",
    "    logging_strategy=script_args.logging_strategy,\n",
    "    logging_steps=script_args.log_steps,\n",
    "    logging_first_step=script_args.logging_first_step,\n",
    "    save_strategy=script_args.save_strategy,\n",
    "    save_steps=script_args.save_steps,\n",
    "    output_dir=script_args.output_dir,\n",
    "    report_to=script_args.report_to,\n",
    "    run_name=script_args.run_name,\n",
    "    save_total_limit=script_args.save_total_limit,\n",
    "    load_best_model_at_end=script_args.load_best_model_at_end,\n",
    "    metric_for_best_model=script_args.metric_for_best_model,\n",
    ")\n",
    "\n",
    "# 5. initialize the DPO trainer\n",
    "\n",
    "if script_args.use_peft:\n",
    "    lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    lora_config = None\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=script_args.beta,\n",
    "    alpha1=script_args.alpha1,\n",
    "    alpha2=script_args.alpha2,\n",
    "    omega1=script_args.omega1,\n",
    "    omega2=script_args.omega2,\n",
    "    S_generated_C_weight=script_args.S_generated_C_weight,\n",
    "    S_generated_D_weight=script_args.S_generated_D_weight,\n",
    "    S_generated_S_weight=script_args.S_generated_S_weight,\n",
    "    S_edited_C_weight=script_args.S_edited_C_weight,\n",
    "    S_edited_I_weight=script_args.S_edited_I_weight,\n",
    "    S_edited_S_weight=script_args.S_edited_S_weight,\n",
    "    output_dir=script_args.output_dir,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=script_args.max_length,\n",
    "    max_prompt_length=script_args.max_prompt_length,\n",
    "    peft_config=lora_config,\n",
    "    alignment_function=script_args.alignment_function,\n",
    ")\n",
    "\n",
    "if script_args.eval_first_step and 0:\n",
    "    print(dpo_trainer.evaluate())\n",
    "\n",
    "# 6. train\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcfd6022-adb3-43fc-8cba-89c246a1999a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52600907-9df7-4743-b7b3-8a2828ca3cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3160 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f87460a-ed40-460e-8532-87f94a7321e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10900 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.00 GB, other allocations: 896.61 MB, max allowed: 18.13 GB). Tried to allocate 348.84 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 6. train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dpo_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "\u001b[1;32m/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=635'>636</a>\u001b[0m \u001b[39m# torch.autograd.set_detect_anomaly(True)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=636'>637</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=637'>638</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=639'>640</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=640'>641</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[1;32m/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=599'>600</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_dpo_data_collator:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=600'>601</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=601'>602</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcompute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=602'>603</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=603'>604</a>\u001b[0m     )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=604'>605</a>\u001b[0m loss, metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_batch_metrics(model, inputs, train_eval\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=606'>607</a>\u001b[0m \u001b[39m# force log the metrics\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=607'>608</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mis_main_process:\n",
      "\u001b[1;32m/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=492'>493</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=493'>494</a>\u001b[0m metrics \u001b[39m=\u001b[39m {}\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=495'>496</a>\u001b[0m (\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=496'>497</a>\u001b[0m     policy_chosen_logps,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=497'>498</a>\u001b[0m     policy_rejected_logps,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=498'>499</a>\u001b[0m     policy_chosen_logits,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=499'>500</a>\u001b[0m     policy_rejected_logits,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=500'>501</a>\u001b[0m     policy_chosen_salt_logps,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=501'>502</a>\u001b[0m     policy_rejected_salt_logps\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=502'>503</a>\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconcatenated_forward(model, batch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=503'>504</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=504'>505</a>\u001b[0m     (\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=505'>506</a>\u001b[0m         reference_chosen_logps,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=506'>507</a>\u001b[0m         reference_rejected_logps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=510'>511</a>\u001b[0m         _,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=511'>512</a>\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcatenated_forward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mref_model, batch)\n",
      "\u001b[1;32m/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=447'>448</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=448'>449</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=449'>450</a>\u001b[0m \u001b[39mWe do this to avoid doing two forward passes, because it's faster for FSDP.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=450'>451</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=451'>452</a>\u001b[0m concatenated_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcatenated_inputs(batch)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=452'>453</a>\u001b[0m all_logits \u001b[39m=\u001b[39m model(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=453'>454</a>\u001b[0m     concatenated_batch[\u001b[39m\"\u001b[39;49m\u001b[39mconcatenated_input_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=454'>455</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mconcatenated_batch[\u001b[39m\"\u001b[39;49m\u001b[39mconcatenated_attention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=455'>456</a>\u001b[0m )\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=457'>458</a>\u001b[0m all_logps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_batch_logps(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=458'>459</a>\u001b[0m     all_logits,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=459'>460</a>\u001b[0m     concatenated_batch[\u001b[39m\"\u001b[39m\u001b[39mconcatenated_labels\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=460'>461</a>\u001b[0m     average_log_prob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=461'>462</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sai.prabhakar/git/LearnFromHumanEdit/SFT-DPO-SALT.ipynb#X22sZmlsZQ%3D%3D?line=462'>463</a>\u001b[0m chosen_logps \u001b[39m=\u001b[39m all_logps[: batch[\u001b[39m\"\u001b[39m\u001b[39mchosen_input_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/salt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:183\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_attn\u001b[39m(\u001b[39mself\u001b[39m, query, key, value, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, head_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 183\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query, key\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[1;32m    186\u001b[0m         attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mfull(\n\u001b[1;32m    187\u001b[0m             [], value\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m, dtype\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    188\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.00 GB, other allocations: 896.61 MB, max allowed: 18.13 GB). Tried to allocate 348.84 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0395494-2760-42a1-86a2-c8dfeae22170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in eval_output_record.items():\n",
    "    df.to_csv(script_args.output_dir + '/' + str(key) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfa700-0e4e-4182-bc17-50a60e3e546c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172922b-e66f-469b-bd44-cd8694782835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
